{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Intro to Python & R for Data Analysis\n",
    "## Lecture: Let's get this data started - Pandas\n",
    "Professor: Mary Kaltenberg\n",
    "\n",
    "Fall 2020\n",
    "\n",
    "contact: mkaltenberg@pace.edu\n",
    "\n",
    "About me: www.mkaltenberg.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "Part 1 (Quickstart Guide):\n",
    "- dataframes\n",
    "- how to import data into a dataframe\n",
    "- merge\n",
    "- drop columns\n",
    "- combine numpy AND pandas\n",
    "- how to export data\n",
    "\n",
    "Part 2 (Detailed Guide):\n",
    "- concat\n",
    "- transform and pivot\n",
    "- groupby\n",
    "- hierarchial indexing\n",
    "- aggregate\n",
    "\n",
    "\n",
    "\n",
    "<img src ='https://media.giphy.com/media/z6xE1olZ5YP4I/giphy.gif' >\n",
    "\n",
    "Pandas technically comes from \"Panel Data.\" I prefer the dancing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the package\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frames + Importing\n",
    "\n",
    "We made it. We are finally at the point at importing real data and doing something with it!  Wahoooo!\n",
    "\n",
    "So, pandas works with dataframes. It's technically an object. Those familiar with object based programming will be familiar with this concept, but well, it is what it seems. It is a thing, and object, that you can manipulate.\n",
    "\n",
    "The first step on this journey is to import data. \n",
    "\n",
    "I've now uploaded data that we can use for today's exercise:\n",
    "\n",
    "- world justice project \n",
    "- general inequality dataset\n",
    "\n",
    "First step is to import the data and name it a variable.\n",
    "\n",
    "The read csv funciton is:\n",
    "`pd.read_csv()`\n",
    "\n",
    "There is also `pd.read_table()` (typically for text files)\n",
    "\n",
    "or import json into a dataframe directly with `pd.read_json()`\n",
    "\n",
    "There is also an option to read stata files:\n",
    "\n",
    "`from pandas import read_stata`\n",
    "\n",
    "`pd.read_stata()`\n",
    "\n",
    "Note of honesty about stata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-11-bcefdf24370c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-bcefdf24370c>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pd.read_csv('C:\\Users\\Gabrielle Martinez\\Documents\\eco590_Data_Analysis_Python_R\\Lecture_6\\ds')\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# use the tab function to read other types of data\n",
    "#pd.read_csv('C:\\Users\\Gabrielle Martinez\\Documents\\eco590_Data_Analysis_Python_R\\Lecture_6\\ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, one note. When you are managing a bunch of files, which you will do. You will want those files organized neatly and not just floating around so it's impossible to find. \n",
    "\n",
    "Generally, I create a folder root that I operate from. And from that point a few folders that I can move throughout the process.\n",
    "\n",
    "To remember my root, I just create a variable and name the path. There are a bunch of ways of doing this (there is a function called path that you can use, but I'm stuck in my ways at this point.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/'\n",
    "ds = '/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/Lecture_6/DS/'\n",
    "\n",
    "#you can name it whatever or use how ever many folders and organization you want. \n",
    "# Just be organized or you will regret it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Users\\mkaltenberg\\Documents\\Data Analysis Python R Lectures\\Data_Analysis_Python_R\\Lecture_6\\DS\\wjp.csv\n"
     ]
    }
   ],
   "source": [
    "#this is another way to organize files - pathlib has a lot of features \n",
    "# that can be useful when you want to recursively open a variety of data files and append them\n",
    "\n",
    "from pathlib import Path, PureWindowsPath\n",
    "p = Path('/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/')\n",
    "\n",
    "#For those in windows, you can also use this to convert filenames\n",
    "#mac uses forward slash and windows uses backslash in directories - this difference causes chaos\n",
    "print(PureWindowsPath(ds+'wjp.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp = pd.read_csv('/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/Lecture_6/DS/wjp.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/Lecture_6/DS/wjp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-775b3270e735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwjp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'wjp.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#ineq = pd.read_csv(ds+'ineq.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#here you can see I am using the variable path names I created so I can easily access the information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# instead of writing the entire path name out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/Lecture_6/DS/wjp.csv'"
     ]
    }
   ],
   "source": [
    "wjp = pd.read_csv(ds+'wjp.csv',index_col=0) \n",
    "#ineq = pd.read_csv(ds+'ineq.csv') \n",
    "\n",
    "#here you can see I am using the variable path names I created so I can easily access the information \n",
    "# instead of writing the entire path name out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp = pd.read_csv(ds+'wjp.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can learn some general things about the dataframe\n",
    "#what columns are in it\n",
    "wjp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what the first few rows looks like \n",
    "wjp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or specify the rows\n",
    "\n",
    "wjp.head(20) # first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouble shooting\n",
    "\n",
    "Some data is trickier, though. \n",
    "\n",
    "Some trouble shooting issues.\n",
    "\n",
    "Not all CSV are created equal. Python reads UTF-8 files. Sometimes, you may have to export your file so that it is 'UTF-8' csv\n",
    "\n",
    "<img src ='utf-8csv.png' width=500 >\n",
    "\n",
    "When you import, not all files are csv. You may have different separators and delimiters.\n",
    "\n",
    "Seperators separate values into different cells. Delimiters create new rows (they mark the end of a row).\n",
    "\n",
    "Often, the easiest thing to do is use the `encoding` option when importing a csv. Usually, `latin` enconding works to fix the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#encoding option in python\n",
    "pd.read_csv('wjp.csv',sep = ',', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/Data_Analysis_Python_R/Lecture_6/DS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you may have trouble importing it and you have no idea why. \n",
    "# A first step is to check what the beginning contents look like:\n",
    "!head 'wjp.csv'\n",
    "\n",
    "#Looking at the first few lines can indicate how it is separated and where/what are the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('wjp.csv',sep = ',')\n",
    "\n",
    "#the optional argument sep will let you pick the particular separator for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('wjp.csv',sep = ',', header = 0)\n",
    "# You may also have to tell pandas what row is the header [remember index 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's take a look at a weird example\n",
    "pd.read_csv('E8081RQI.TXT')\n",
    "# could also import with pd.read_table('E8081RQI.TXT')\n",
    "#it's not separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head 'E8081RQI.TXT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('E8081RQI.TXT', sep = '\\s+')\n",
    "\n",
    "# \\t = tab\n",
    "# \\s = space\n",
    "# \\s+ = many spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can choose which columns we to include and import without a header\n",
    "test_data = pd.read_csv('E8081RQI.TXT', sep = '\\s+', usecols=(range(0,5)), header=None)\n",
    "\n",
    "#We can rename columns\n",
    "test_data.columns='dataset','variable','population','GDP','Income'\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also drop anything in the file that is missing\n",
    "pd.read_csv('E8081RQI.TXT', sep = '\\s+').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful - it will drop entire rows that have one na\n",
    "test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or we can replace missing values with whatever we want\n",
    "pd.read_csv('E8081RQI.TXT', sep = '\\s+').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('E8081RQI.TXT', sep='\\s+')\n",
    "test_data[test_data.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes you may get a mysterious 'Unnamed: 0' - often this is from python and it's an index (usually the first column)\n",
    "You can do away with this by setting this column as the index\n",
    " \n",
    "`pd.read_csv(filename, index_col = 0)`\n",
    "\n",
    "Also, you can import from a clipboard by copy and pasting (but there can be errors, so be careful)\n",
    "\n",
    "`pd.read_clipboard()`\n",
    "\n",
    "Or from a pdf\n",
    "\n",
    "``` python\n",
    "from tabula import read_pdf\n",
    "df = read_pdf('test.pdf', pages='all')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "Often you'll want to combine datasets. Typically, you will pool together a variety of datasets into one dataframe.\n",
    "\n",
    "Currently we have two dataframes: wjp and ineq\n",
    "\n",
    "We will merge the two. There are different ways that you can merge.\n",
    "\n",
    "Other \"adding together\" dataframes include: `pd.append()` (hey you know that!) and `pd.concat()`\n",
    "\n",
    "They can be useful in different scenarios.\n",
    "\n",
    "By far, though, `pd.merge()` is your BFF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ineq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I need to take a look at the column names and see what I want to merge by. In this case it is country.\n",
    "pd.merge(wjp, ineq, left_on=['Country','year'],right_on=['country','year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! What happened? What is this wizardry?\n",
    "\n",
    " The first two are the dataframes you want to merge. Only two can be merged at a time.\n",
    " \n",
    "left_on is the key that will match with the left dataframe\n",
    "right_on is the key that will match with the right dataframe\n",
    "pandas merge will look for EXACT matched between the keys\n",
    "\n",
    "how it matches depends onthe argument \"how\"\n",
    "\n",
    "In this example, it will only look at the keys on the left and will match with items on the right so long as it is in the key of the left\n",
    "\n",
    "By default merge does an 'inner' join; the keys in the result are the intersection (if you didn't put \"how\"). In general, don't ever rely on the default. You'll lose stuff - be aware of how you merge.\n",
    "\n",
    "There are four ways to join\n",
    "<img src =\"merge_joins.png\">\n",
    "\n",
    "Remember union vs. intersection\n",
    "<img src =\"uion_intersection.png\">\n",
    "\n",
    "From this [great website](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) to check out\n",
    "\n",
    "\n",
    "Which we will do right now.\n",
    "\n",
    "\n",
    "<img src = \"merge_options.png\">\n",
    "From P4DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas + Stuff\n",
    "\n",
    "Everything you learned so far can be applied to your dataframe. EVERYTHING.\n",
    "\n",
    "Which means, this is the part of the class where you will go in breakout groups and be totally lost for like 5-10 minutes and then figure out how to do it. \n",
    "\n",
    "First, some hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp_ineq = pd.merge(wjp, ineq, left_on=['Country','year'],right_on=['country','year'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp_ineq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter \n",
    "data = wjp_ineq[['Country', 'Region','Income Group', 'year', 'isocode',\n",
    "                 'factor1', 'factor2','factor3','factor4', 'factor5','factor6',\n",
    "                 'factor7','factor8','gini', 'population']]\n",
    "\n",
    "#this filters in the same way that we have seen filtering in the past\n",
    "\n",
    "# when I import and merge, I usually leave the original variable the same \n",
    "# so that I can always reference it if I make a mistake in merging or something else\n",
    "\n",
    "#The double brackets mean, keep everything that is within the identified columns. You choose the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can list out the unique values of any column\n",
    "wjp_ineq['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp_ineq['Country'].nunique()\n",
    "#or count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And filter\n",
    "wjp_ineq[wjp_ineq['Country']=='Uruguay']\n",
    "\n",
    "#filtering is the same way I have been showing you all along - boolean searches/ \n",
    "# You can filter by values as well or anything that I taught you before \n",
    "# (just be sure it's the same type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wjp_ineq[wjp_ineq['gini']>.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or any of the numpy functions\n",
    "wjp_ineq['gini'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get fancy in our filtering and cleaning data\n",
    "\n",
    "#for each cell in this object I have, data, find strings that contain the values 'factor'\n",
    "# and store that information in factor_frame\n",
    "factor_frame = [x for x in list(data) if x.startswith('factor')]\n",
    "#here's another way to do the same thing\n",
    "factor_frame = [x for x in list(data) if 'factor' in x]\n",
    "\n",
    "# Here's a subset of the data using the list we created from our loop\n",
    "#We want to include a few other columns besides factor by using extend to add items in the list we created\n",
    "factor_frame.extend(['Country', 'Income Group', 'Region', 'gini', 'population'])\n",
    "\n",
    "factor_frame = data[factor_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many countries? (temporarily store this value for future use)\n",
    "nc = data['isocode'].nunique()\n",
    "#How many observations? \n",
    "print('Number of Observations in ds:', len(data)) #print in the output with string and some information you just calculated in your output box\n",
    "#How many years? \n",
    "print('Number of years in ds:', data['year'].nunique())\n",
    "#drop a column that is unneeded\n",
    "c = data.drop(['year'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows instead of columns\n",
    "#resetting index to Region so that I can drop all rows in the index that are 'Eastern Europe & Central Asia'\n",
    "c = c.set_index('Region')\n",
    "c.drop('Eastern Europe & Central Asia',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can also reset indexes\n",
    "c = c.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_frame['total'] = factor_frame['factor1']+factor_frame['factor2'] +factor_frame['factor3']+factor_frame['factor4']+factor_frame['factor5']+factor_frame['factor6']+factor_frame['factor7']+factor_frame['factor8']\n",
    "# create a variable that adds up all of the factors        \n",
    "factor_frame['factor_avg'] =  factor_frame['total'].mean()  #create a new variable with the mean of the toal\n",
    "factor_frame['total'] = factor_frame['total'].astype(float) #store the column total as a floar\n",
    "# factor_frame = factor_frame.drop(['total'],1) #drop the column total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "We can easily export dataframes at any time with:\n",
    "\n",
    "`df.to_csv(filename.csv)`\n",
    "\n",
    "\n",
    "I almost always use the option argument of index to set it to false. Typically, I don't need to index to travel\n",
    "\n",
    "`dataframe.to_csv('filename.csv', index = False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/Users/mkaltenberg/Documents/Data Analysis Python R Lectures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will export to the file location that you are currently in.\n",
    "# So, be careful - know where you are in your directory.\n",
    "factor_frame.to_csv('factors_frame_avg.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I hope all of that time spent on python functions are coming together for some magic.\n",
    "\n",
    "### Now it's your turn!\n",
    "\n",
    "<img src ='https://media.giphy.com/media/citBl9yPwnUOs/giphy.gif' width = 300>\n",
    "\n",
    "More useful tips from pandas at this [website](https://www.dataschool.io/python-pandas-tips-and-tricks/#readingfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice exercise\n",
    "# get to this point:\n",
    "\n",
    "wjp = pd.read_csv('wjp.csv') \n",
    "ineq = pd.read_csv('ineq.csv') \n",
    "wjp_ineq = pd.merge(wjp, ineq, left_on=['Country','year'],right_on=['country','year'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter out the dataset to show only data from the region 'Sub-Saharan Africa'\n",
    "2. How many countries are in the region?\n",
    "3. Calculate the average gini of the region.\n",
    "4. What's the maximum population in the region? What's the countries name?\n",
    "5. Can you do a for loop that can do this calculation for all of the regions?\n",
    "6. Export the filtered dataset of 1 (only countries that are from the region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
